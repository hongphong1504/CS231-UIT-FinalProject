{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":620468,"sourceType":"datasetVersion","datasetId":304145},{"sourceId":12101524,"sourceType":"datasetVersion","datasetId":7535340}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Phát hiện người sử dụng Fine-tuned Faster R-CNN","metadata":{}},{"cell_type":"markdown","source":"# Import thư viện","metadata":{}},{"cell_type":"code","source":"# Import thư viện\nimport os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms import functional as F\nfrom torchvision.datasets import CocoDetection\nfrom torchvision import transforms\nimport torchvision\nimport matplotlib.pyplot as plt\nfrom pycocotools.cocoeval import COCOeval\nfrom pycocotools.coco import COCO\nimport json\nfrom PIL import Image, ImageDraw, ImageFont\nimport optuna\nfrom pycocotools.coco import COCO\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as T\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nimport matplotlib.pyplot as plt\nimport albumentations as A\nimport cv2\nfrom pathlib import Path\nimport shutil\nimport requests\nfrom io import BytesIO\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport tempfile\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:25:11.773311Z","iopub.execute_input":"2025-06-19T06:25:11.773600Z","iopub.status.idle":"2025-06-19T06:25:11.780576Z","shell.execute_reply.started":"2025-06-19T06:25:11.773577Z","shell.execute_reply":"2025-06-19T06:25:11.779604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Định nghĩa class Dataset và khởi tạo các loader","metadata":{}},{"cell_type":"code","source":"class CocoDataset(CocoDetection):\n    \"\"\"\n    Dataset tổng quát cho dữ liệu COCO, hỗ trợ cả chế độ không augmentation và có augmentation.\n    \n    Args:\n        img_folder (str): Thư mục chứa hình ảnh.\n        ann_file (str): Đường dẫn đến file annotation COCO (JSON).\n        transforms (callable, optional): Biến đổi cho hình ảnh (thường dùng với torchvision).\n        augmentation (callable, optional): Pipeline tăng cường dữ liệu (thường dùng với Albumentations).\n    \"\"\"\n    def __init__(self, img_folder, ann_file, augmentation=None):\n        super().__init__(img_folder, ann_file)\n        self._transforms = transforms\n        self._augmentation = augmentation\n        self._to_tensor = T.ToTensor()\n\n    def __getitem__(self, idx):\n        img, targets = super().__getitem__(idx)\n        \n        # Chuyển đổi annotations COCO sang định dạng Faster R-CNN\n        boxes = []\n        labels = []\n        for t in targets:\n            xmin = t['bbox'][0]\n            ymin = t['bbox'][1]\n            xmax = xmin + t['bbox'][2]\n            ymax = ymin + t['bbox'][3]\n            boxes.append([xmin, ymin, xmax, ymax])\n            labels.append(t['category_id'])\n        \n        if self._augmentation:\n            # Chế độ augmentation (Albumentations)\n            img = np.array(img)  # Chuyển PIL Image sang numpy array\n            augmented = self._augmentation(image=img, bboxes=boxes, class_labels=labels)\n            img = augmented['image']\n            boxes = augmented['bboxes']\n            labels = augmented['class_labels']\n            # Chuyển lại thành tensor cho Faster R-CNN\n            img = torch.tensor(img.transpose(2, 0, 1), dtype=torch.float32) / 255.0\n        else:\n            # Chế độ không augmentation\n            img = self._to_tensor(img)\n                \n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.as_tensor(labels, dtype=torch.int64)\n        target = {\n            \"boxes\": boxes,\n            \"labels\": labels,\n            \"image_id\": torch.tensor([idx])\n        }\n        \n        return img, target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:25:14.667445Z","iopub.execute_input":"2025-06-19T06:25:14.667796Z","iopub.status.idle":"2025-06-19T06:25:14.676245Z","shell.execute_reply.started":"2025-06-19T06:25:14.667772Z","shell.execute_reply":"2025-06-19T06:25:14.675527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"aug_train_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),  # Horizontal flip with 50% probability\n    A.Rotate(limit=15, p=0.2),  # Rotate ±30 degrees with 30% probability\n    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.3),  # Adjust brightness and contrast\n    A.GaussNoise(p=0.1),  # Add Gaussian noise\n    A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=10, p=0.3),  # Shift, scale, rotate\n], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['class_labels']))\n\ntrain_dataset = CocoDataset(\n    img_folder='/kaggle/input/inriaperson/Train/JPEGImages',\n    ann_file='/kaggle/input/inria-coco-format/coco_train.json'\n)\n\ntest_dataset = CocoDataset(\n    img_folder='/kaggle/input/inriaperson/Test/JPEGImages',\n    ann_file='/kaggle/input/inria-coco-format/coco_test.json'\n)\n\naug_train_dataset = CocoDataset(\n    img_folder='/kaggle/input/inriaperson/Train/JPEGImages',  \n    ann_file='/kaggle/input/inria-coco-format/coco_train.json', \n    augmentation = aug_train_transform\n)\n\naug_train_loader = DataLoader(aug_train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:25:17.158447Z","iopub.execute_input":"2025-06-19T06:25:17.159113Z","iopub.status.idle":"2025-06-19T06:25:17.187475Z","shell.execute_reply.started":"2025-06-19T06:25:17.159088Z","shell.execute_reply":"2025-06-19T06:25:17.186752Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Định nghĩa các hàm phụ (hàm đánh giá, hàm load checkpoint và hàm vẽ đồ thị loss)","metadata":{}},{"cell_type":"code","source":"def evaluate_map(model, dataloader, ann_file, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n    \"\"\"\n    Tính mAP cho mô hình Faster R-CNN trên tập dữ liệu COCO.\n    \n    Args:\n        model: Mô hình Faster R-CNN đã được huấn luyện.\n        dataloader: DataLoader cho tập kiểm tra/valid.\n        ann_file: Đường dẫn đến file annotation COCO (JSON).\n        device: Thiết bị tính toán (cuda hoặc cpu).\n    \n    Returns:\n        float: Giá trị mAP (COCO-style, trung bình IoU từ 0.5 đến 0.95).\n    \"\"\"\n    model.eval()\n    model.to(device)\n    \n    # Tải COCO ground truth\n    coco_gt = COCO(ann_file)\n    \n    # Danh sách lưu trữ dự đoán\n    predictions = []\n    image_ids = []\n    \n    with torch.no_grad():\n        for images, targets in dataloader:\n            images = [img.to(device) for img in images]\n            # Dự đoán từ mô hình\n            outputs = model(images)\n            \n            for i, output in enumerate(outputs):\n                image_id = targets[i][\"image_id\"].item() if \"image_id\" in targets[i] else len(image_ids)\n                image_ids.append(image_id)\n                \n                # Lấy boxes, scores, labels từ output\n                boxes = output[\"boxes\"].cpu().numpy()  # [x_min, y_min, x_max, y_max]\n                scores = output[\"scores\"].cpu().numpy()\n                labels = output[\"labels\"].cpu().numpy()\n                \n                # Chuyển đổi boxes sang định dạng COCO [x, y, width, height]\n                boxes_coco = np.zeros_like(boxes)\n                boxes_coco[:, 0] = boxes[:, 0]  # x_min\n                boxes_coco[:, 1] = boxes[:, 1]  # y_min\n                boxes_coco[:, 2] = boxes[:, 2] - boxes[:, 0]  # width\n                boxes_coco[:, 3] = boxes[:, 3] - boxes[:, 1]  # height\n                \n                # Lưu dự đoán vào danh sách\n                for box, score, label in zip(boxes_coco, scores, labels):\n                    predictions.append({\n                        \"image_id\": int(image_id),\n                        \"category_id\": int(label),  # Class ID\n                        \"bbox\": box.tolist(),       # [x, y, width, height]\n                        \"score\": float(score)       # Confidence score\n                    })\n    \n    # Tạo COCO object cho dự đoán\n    coco_dt = coco_gt.loadRes(predictions)\n    \n    # Khởi tạo COCOeval\n    coco_eval = COCOeval(coco_gt, coco_dt, iouType=\"bbox\")\n    \n    # Tính mAP\n    coco_eval.evaluate()\n    coco_eval.accumulate()\n    coco_eval.summarize()\n    \n    # Lấy mAP@IoU=0.5:0.95 (COCO default)\n    map_score = coco_eval.stats[0]  # mAP@IoU=0.5:0.95\n    \n    return map_score            \n    \n\n\ndef load_checkpoint(model, checkpoint_path, device, checkpoint_input_dir='/kaggle/input/inria-coco-format'):\n    \"\"\"\n    Load checkpoint state cho model và optimizer.\n    \n    Args:\n        model: Mô hình Faster RCNN.\n        checkpoint_path: Tên file checkpoint.\n        device: Thiết bị tính toán.\n        checkpoint_input_dir: đường dẫn đến folder chứa file checkpoint.\n        \n    Returns:\n        Số epoch, train_losses, test_losses model tại checkpoint.\n        \n    \"\"\"\n    checkpoint_full_path = os.path.join(checkpoint_input_dir, checkpoint_path)\n    if os.path.exists(checkpoint_full_path):\n        checkpoint = torch.load(checkpoint_full_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        epoch = checkpoint['epoch']\n        train_losses = checkpoint['train_losses']\n        test_losses = checkpoint['test_losses']\n        print(f\"Loaded checkpoint from {checkpoint_full_path}, resuming from epoch {epoch + 1}\")\n        return epoch, train_losses, test_losses\n    else:\n        print(f\"No checkpoint found at {checkpoint_full_path}\")\n        return 0, [], []\n\ndef plot_losses(train_losses, test_losses, output_dir='/kaggle/working'):\n    \"\"\"\n    Vẽ đồ thị biểu hiện giá trị của train_loss và test_loss qua các epoch\n\n    Args:\n        train_losses: mảng chứa giá trị của các loss trong quá trình training.\n        test_losses: mảng chứa giá trị của các loss trong quá trình testing.\n\n    Returns:\n        None\n    \"\"\"\n    plt.figure(figsize=(10, 6))\n    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')\n    if any(l != float('nan') for l in test_losses):\n        plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss', marker='s')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Train and Test Loss over Epochs')\n    plt.legend()\n    plt.grid(True)\n    plot_path = os.path.join(output_dir, 'loss_plot.png')\n    plt.savefig(plot_path)\n    plt.close()\n    print(f\"Saved loss plot at {plot_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:25:20.684059Z","iopub.execute_input":"2025-06-19T06:25:20.684353Z","iopub.status.idle":"2025-06-19T06:25:20.699330Z","shell.execute_reply.started":"2025-06-19T06:25:20.684332Z","shell.execute_reply":"2025-06-19T06:25:20.698341Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Định nghĩa hàm train","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_dataloader, test_dataloader, optimizer, num_epochs, device, \n                checkpoint_dir='/kaggle/working', checkpoint_input_dir='/kaggle/input/inria-coco-format', \n                epochs_per_run=2, resume_from_checkpoint=None, patience=5, min_delta=0.001):\n    model.to(device)\n    \n    # Khởi tạo lịch sử loss\n    train_losses = []\n    test_losses = []\n    start_epoch = 0\n    best_test_loss = float('inf')\n    epochs_no_improve = 0\n    \n    # Khởi tạo scheduler ReduceLROnPlateau\n    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3, verbose=True)\n    \n    # Tải checkpoint nếu có\n    if resume_from_checkpoint and os.path.exists(os.path.join(checkpoint_input_dir, resume_from_checkpoint)):\n        checkpoint_path = os.path.join(checkpoint_input_dir, resume_from_checkpoint)\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        model.load_state_dict(checkpoint['model_state_dict'])\n        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n        start_epoch = checkpoint['epoch'] + 1\n        train_losses = checkpoint['train_losses']\n        test_losses = checkpoint['test_losses']\n        best_test_loss = checkpoint.get('best_test_loss', float('inf'))\n        epochs_no_improve = checkpoint.get('epochs_no_improve', 0)\n        print(f\"Resumed from checkpoint at {checkpoint_path}, starting from epoch {start_epoch}\")\n    elif resume_from_checkpoint:\n        print(f\"No checkpoint found at {os.path.join(checkpoint_input_dir, resume_from_checkpoint)}\")\n    \n    # Tính tổng số ảnh trong dataset\n    train_dataset_size = len(train_dataloader.dataset)\n    test_dataset_size = len(test_dataloader.dataset)\n    \n    # Vòng lặp qua các epoch\n    for epoch in range(start_epoch, num_epochs):\n        # Training phase\n        model.train()\n        total_train_loss = 0\n        images_processed = 0\n        train_progress = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", \n                             leave=False, total=len(train_dataloader))\n        for images, targets in train_progress:\n            batch_size = len(images)\n            images_processed += batch_size\n            images = [img.to(device) for img in images]\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n            \n            loss_dict = model(images, targets)\n            losses = sum(loss for loss in loss_dict.values())\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n            \n            total_train_loss += losses.item()\n            train_progress.set_postfix({\n                'batch_loss': f\"{losses.item():.4f}\",\n                'images': f\"{images_processed}/{train_dataset_size}\"\n            })\n        \n        avg_train_loss = total_train_loss / len(train_dataloader)\n        train_losses.append(avg_train_loss)\n        \n        # Testing phase\n        model.eval()\n        total_test_loss = 0\n        images_processed = 0\n        test_progress = tqdm(test_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} [Test]\", \n                            leave=False, total=len(test_dataloader))\n        with torch.no_grad():\n            for images, targets in test_progress:\n                batch_size = len(images)\n                images_processed += batch_size\n                images = [img.to(device) for img in images]\n                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n                \n                # Tạm thời chuyển sang train mode để tính loss\n                model.train()\n                loss_dict = model(images, targets)\n                model.eval()\n                \n                if isinstance(loss_dict, dict):\n                    losses = sum(loss for loss in loss_dict.values())\n                    total_test_loss += losses.item()\n                else:\n                    print(f\"Warning: Loss not computed for batch in test phase, got {type(loss_dict)}\")\n                    total_test_loss += 0\n                \n                test_progress.set_postfix({\n                    'batch_loss': f\"{losses.item():.4f}\" if isinstance(loss_dict, dict) else \"N/A\",\n                    'images': f\"{images_processed}/{test_dataset_size}\"\n                })\n        \n        avg_test_loss = total_test_loss / len(test_dataloader) if total_test_loss > 0 else float('nan')\n        test_losses.append(avg_test_loss)\n        \n        # Cập nhật scheduler\n        if avg_test_loss != float('nan'):\n            scheduler.step(avg_test_loss)\n        \n        # In kết quả\n        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\" if avg_test_loss != float('nan') else 'N/A')\n        \n        # Lưu checkpoint\n        checkpoint = {\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'train_losses': train_losses,\n            'test_losses': test_losses,\n            'best_test_loss': best_test_loss,\n            'epochs_no_improve': epochs_no_improve\n        }\n        # Lưu checkpoint sau mỗi epochs_per_run hoặc ở epoch cuối\n        if (epoch + 1) % epochs_per_run == 0 or epoch + 1 == num_epochs:\n            checkpoint_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch+1}.pth')\n            torch.save(checkpoint, checkpoint_path)\n            print(f\"Saved checkpoint at {checkpoint_path}\")\n            \n            # Lưu lịch sử loss vào JSON\n            loss_history = {\n                'train_losses': train_losses,\n                'test_losses': test_losses\n            }\n            loss_history_path = os.path.join(checkpoint_dir, 'loss_history.json')\n            with open(loss_history_path, 'w') as f:\n                json.dump(loss_history, f, indent=4)\n            print(f\"Saved loss history at {loss_history_path}\")\n        \n        # Early Stopping\n        if avg_test_loss != float('nan') and avg_test_loss < best_test_loss - min_delta:\n            best_test_loss = avg_test_loss\n            epochs_no_improve = 0\n            torch.save(checkpoint, os.path.join(checkpoint_dir, 'best_model.pth'))\n            print(f\"Saved best model with Test Loss: {best_test_loss:.4f}\")\n        elif avg_test_loss != float('nan'):\n            epochs_no_improve += 1\n        \n        if epochs_no_improve >= patience:\n            print(f\"Early stopping triggered after {epochs_no_improve} epochs without improvement\")\n            break\n    \n    # Vẽ biểu đồ\n    plot_losses(train_losses, test_losses, checkpoint_dir)\n    \n    return train_losses, test_losses","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:25:24.418779Z","iopub.execute_input":"2025-06-19T06:25:24.419514Z","iopub.status.idle":"2025-06-19T06:25:24.436971Z","shell.execute_reply.started":"2025-06-19T06:25:24.419488Z","shell.execute_reply":"2025-06-19T06:25:24.436329Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model và đánh giá model với *trainable_backbone_layers = 3*\n","metadata":{}},{"cell_type":"code","source":"ft_model = fasterrcnn_resnet50_fpn(pretrained=True, trainable_backbone_layers=3)\nft_in_features = ft_model.roi_heads.box_predictor.cls_score.in_features\nft_model.roi_heads.box_predictor = FastRCNNPredictor(ft_in_features, 2)\nft_model.to(device)\n\noptimizer = torch.optim.SGD(\n    ft_model.parameters(),\n    lr=0.001,\n    weight_decay= 0.0005,\n    momentum=0.9\n)\n    \nnum_epochs = 100\nepochs_per_run = 5\ntrain_losses, test_losses = train_model(\nft_model, train_loader, test_loader, optimizer, num_epochs, device,\ncheckpoint_dir='/kaggle/working',\ncheckpoint_input_dir='/kaggle/input/inria-coco-format',\nepochs_per_run=epochs_per_run, resume_from_checkpoint=None)\n\nprint(evaluate_map(ft_model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train model và đánh giá model với *trainable_backbone_layers = 0* và dùng augmented data","metadata":{}},{"cell_type":"code","source":"# Train model và đánh giá model với trainable_backbone_layers = 3 và dùng augmented data\naug_model = fasterrcnn_resnet50_fpn(pretrained=True, trainable_backbone_layers=0)\naug_in_features = aug_model.roi_heads.box_predictor.cls_score.in_features\naug_model.roi_heads.box_predictor = FastRCNNPredictor(aug_in_features, 2)\naug_model.to(device)\n\naug_optimizer = torch.optim.SGD(\n    aug_model.parameters(),\n    lr=0.001,\n    weight_decay= 0.0005,\n    momentum=0.9\n)\n\nnum_epochs = 100\nepochs_per_run = 5\ntrain_losses, test_losses = train_model(\naug_model, aug_train_loader, test_loader, aug_optimizer, num_epochs, device,\ncheckpoint_dir='/kaggle/working',\ncheckpoint_input_dir='/kaggle/input/inria-coco-format',\nepochs_per_run=epochs_per_run, resume_from_checkpoint=None)\n\nprint(evaluate_map(aug_model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Đánh giá các model\n\nChúng em đã thực hiện train các model và lưu checkpoint vào những file cụ thể như sau:\n- Model được train dùng dataset tăng cường và *trainable_backbone_layers = 0*: aug_best_model_0.pth\n- Model được train không dùng dataset tăng cường và *trainable_backbone_layers = 0*: best_model_0.pth\n- Model được train không dùng dataset tăng cường và *trainable_backbone_layers = 3*: best_model_3.pth\n- Model được train không dùng dataset tăng cường và *trainable_backbone_layers = 5*: best_model_5.pth","metadata":{}},{"cell_type":"code","source":"# Đánh giá pretrained FasterRCNN\npretrained_model = fasterrcnn_resnet50_fpn(pretrained = True)\npretrained_model.to(device)\nprint(evaluate_map(pretrained_model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:25:30.955942Z","iopub.execute_input":"2025-06-19T06:25:30.956206Z","iopub.status.idle":"2025-06-19T06:26:19.239494Z","shell.execute_reply.started":"2025-06-19T06:25:30.956188Z","shell.execute_reply":"2025-06-19T06:26:19.238787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Khai báo mô hình với lớp đầu ra là 2 (người hay nền)\nmodel = fasterrcnn_resnet50_fpn(pretrained = True)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\nmodel.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đánh giá model dùng dataset tăng cường và trainable_backbone_layers = 0\nload_checkpoint(model, 'aug_best_model_0.pth', device, checkpoint_input_dir='/kaggle/input/inria-coco-format')\nprint(evaluate_map(model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:43:36.793447Z","iopub.execute_input":"2025-06-19T06:43:36.794223Z","iopub.status.idle":"2025-06-19T06:44:19.528240Z","shell.execute_reply.started":"2025-06-19T06:43:36.794188Z","shell.execute_reply":"2025-06-19T06:44:19.527499Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đánh giá model không dùng dataset tăng cường và trainable_backbone_layers = 0\nload_checkpoint(model, 'best_model_0.pth', device, checkpoint_input_dir='/kaggle/input/inria-coco-format')\nprint(evaluate_map(model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:45:14.591602Z","iopub.execute_input":"2025-06-19T06:45:14.591923Z","iopub.status.idle":"2025-06-19T06:45:58.100523Z","shell.execute_reply.started":"2025-06-19T06:45:14.591901Z","shell.execute_reply":"2025-06-19T06:45:58.099649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đánh giá model không dùng dataset tăng cường và trainable_backbone_layers = 3\nload_checkpoint(model, 'best_model_3.pth', device, checkpoint_input_dir='/kaggle/input/inria-coco-format')\nprint(evaluate_map(model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:45:58.101916Z","iopub.execute_input":"2025-06-19T06:45:58.102131Z","iopub.status.idle":"2025-06-19T06:46:41.283359Z","shell.execute_reply.started":"2025-06-19T06:45:58.102113Z","shell.execute_reply":"2025-06-19T06:46:41.282621Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Đánh giá model không dùng dataset tăng cường và trainable_backbone_layers = 5\nload_checkpoint(model, 'best_model_5.pth', device, checkpoint_input_dir='/kaggle/input/inria-coco-format')\nprint(evaluate_map(model, test_loader, \"/kaggle/input/inria-coco-format/coco_test.json\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:46:41.284181Z","iopub.execute_input":"2025-06-19T06:46:41.284382Z","iopub.status.idle":"2025-06-19T06:47:24.565829Z","shell.execute_reply.started":"2025-06-19T06:46:41.284366Z","shell.execute_reply":"2025-06-19T06:47:24.565069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Như đã trình bày ở phần thuyết trình, theo quan sát, tụi em thấy model không dùng dataset tăng cường và có tham số trainable_backbone_layers = 3 là có kết quả tốt nhất theo tiêu chí đánh giá đặt ra trong bài, nên tụi em sẽ sử dụng nó để thực hiện các demo.","metadata":{}},{"cell_type":"code","source":"load_checkpoint(model, 'best_model_3.pth', device, checkpoint_input_dir='/kaggle/input/inria-coco-format')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:49:00.867253Z","iopub.execute_input":"2025-06-19T06:49:00.867540Z","iopub.status.idle":"2025-06-19T06:49:01.230507Z","shell.execute_reply.started":"2025-06-19T06:49:00.867521Z","shell.execute_reply":"2025-06-19T06:49:01.229887Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Thực nghiệm","metadata":{}},{"cell_type":"markdown","source":"Ở phần này, tụi em thực hiện vẽ ground truth bbox, đồng thời vẽ các predicted bbox trên tất cả các ảnh (ảnh train và test trong dataset INRIA) và giải nén thành 2 file zip: train_output.zip(chứa các ảnh train đã được vẽ các bbox) và test_output.zip(chứa các ảnh test đã được vẽ các bbox).\n","metadata":{}},{"cell_type":"code","source":"def load_coco_data(json_path):\n    \"\"\"Load COCO format annotations from a JSON file.\"\"\"\n    with open(json_path, 'r') as f:\n        coco_data = json.load(f)\n    return coco_data\n\ndef get_image_paths_and_annotations(coco_data, image_dir):\n    \"\"\"Extract image paths and corresponding annotations.\"\"\"\n    image_info = {img['id']: img for img in coco_data['images']}\n    annotations = coco_data['annotations']\n    image_paths = []\n    image_annotations = {}\n    \n    for img_id, img in image_info.items():\n        image_path = os.path.join(image_dir, img['file_name'])\n        image_paths.append(image_path)\n        image_annotations[img['file_name']] = [ann for ann in annotations if ann['image_id'] == img_id]\n    \n    return image_paths, image_annotations\n\ndef draw_bboxes(image, gt_bboxes, pred_bboxes, output_path):\n    \"\"\"Draw ground truth (green) and predicted (red) bounding boxes with confidence scores.\"\"\"\n    draw = ImageDraw.Draw(image)\n    \n    # Try to load a font, fall back to default if not available\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 15)\n    except:\n        font = ImageFont.load_default()\n    \n    # Draw ground truth boxes (green)\n    for bbox in gt_bboxes:\n        x, y, w, h = bbox['bbox']\n        draw.rectangle((x, y, x + w, y + h), outline='green', width=2)\n    \n    # Draw predicted boxes (red) with confidence scores\n    for bbox, score in zip(pred_bboxes['boxes'], pred_bboxes['scores']):\n        x1, y1, x2, y2 = bbox.tolist()\n        draw.rectangle((x1, y1, x2, y2), outline='red', width=2)\n        draw.text((x1, y1 - 15), f'{score:.2f}', fill='red', font=font)\n    \n    # Save the output image\n    image.save(output_path)\n\ndef process_dataset(model, image_paths, annotations, image_dir, output_dir):\n    \"\"\"Process images in a dataset, draw bboxes, and save results.\"\"\"\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    os.makedirs(output_dir, exist_ok=True)\n    \n    for img_path in image_paths:\n        img_name = os.path.basename(img_path)\n        img = Image.open(img_path).convert('RGB')\n        img_tensor = F.to_tensor(img).unsqueeze(0).to(device)\n        \n        # Get predictions\n        with torch.no_grad():\n            predictions = model(img_tensor)[0]\n        \n        # Get ground truth bboxes\n        gt_bboxes = annotations.get(img_name, [])\n        \n        # Draw bboxes and save\n        output_path = os.path.join(output_dir, img_name)\n        draw_bboxes(img, gt_bboxes, predictions, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:19:46.945386Z","iopub.execute_input":"2025-06-06T11:19:46.946074Z","iopub.status.idle":"2025-06-06T11:19:46.956716Z","shell.execute_reply.started":"2025-06-06T11:19:46.946054Z","shell.execute_reply":"2025-06-06T11:19:46.955895Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_img_dir = '/kaggle/input/inriaperson/Train/JPEGImages'\ntest_img_dir = '/kaggle/input/inriaperson/Test/JPEGImages'\ntrain_json_path = '/kaggle/input/inria-coco-format/coco_train.json'\ntest_json_path = '/kaggle/input/inria-coco-format/coco_test.json'\n\n# Load COCO annotations\ntrain_coco = load_coco_data(train_json_path)\ntest_coco = load_coco_data(test_json_path)\n\n# Get image paths and annotations\ntrain_image_paths, train_annotations = get_image_paths_and_annotations(train_coco, train_img_dir)\ntest_image_paths, test_annotations = get_image_paths_and_annotations(test_coco, test_img_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:19:55.739287Z","iopub.execute_input":"2025-06-06T11:19:55.739983Z","iopub.status.idle":"2025-06-06T11:19:55.794956Z","shell.execute_reply.started":"2025-06-06T11:19:55.739957Z","shell.execute_reply":"2025-06-06T11:19:55.794457Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_output_dir = '/kaggle/working/train_output'\ntest_output_dir = '/kaggle/working/test_output'\n\nnormal_model.eval()\n\n# Process train and test datasets\nprocess_dataset(normal_model, train_image_paths, train_annotations, train_img_dir, train_output_dir)\nprocess_dataset(normal_model, test_image_paths, test_annotations, test_img_dir, test_output_dir)\n\n# Zip the output directories\nshutil.make_archive('/kaggle/working/train_output', 'zip', train_output_dir)\nshutil.make_archive('/kaggle/working/test_output', 'zip', test_output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:20:20.041865Z","iopub.execute_input":"2025-06-06T11:20:20.042101Z","iopub.status.idle":"2025-06-06T11:25:19.838103Z","shell.execute_reply.started":"2025-06-06T11:20:20.042086Z","shell.execute_reply":"2025-06-06T11:25:19.837297Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hàm vẽ predicted bbox trên một ảnh bất kỳ","metadata":{}},{"cell_type":"code","source":"# Theo đường dẫn file ảnh\ndef draw_predicted_bbox_file(image_path, model, output_path):\n    \"\"\"\n    Draw predicted bounding boxes (red) with confidence scores on a single image,\n    display the image, and save it to output_path.\n    \n    Args:\n        image_path (str): Path to the input image.\n        model: Trained Faster R-CNN model.\n        output_path (str): Path to save the output image.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    # Load and preprocess image\n    image = Image.open(image_path).convert('RGB')\n    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        predictions = model(image_tensor)[0]\n    \n    # Draw predicted boxes\n    draw = ImageDraw.Draw(image)\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 15)\n    except:\n        font = ImageFont.load_default()\n    \n    for bbox, score in zip(predictions['boxes'], predictions['scores']):\n        x1, y1, x2, y2 = bbox.tolist()\n        draw.rectangle((x1, y1, x2, y2), outline='red', width=2)\n        draw.text((x1, y1 - 15), f'{score:.2f}', fill='red', font=font)\n    \n    # Save output image\n    image.save(output_path)\n    \n    # Display image\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.array(image))\n    plt.axis('off')\n    plt.show()\n    \n# Theo link URL của ảnh\ndef draw_predicted_bbox(url, model, output_path):\n    \"\"\"\n    Draw predicted bounding boxes (red) with confidence scores on an image from a URL,\n    display the image, and save it to output_path.\n    \n    Args:\n        url (str): URL of the input image.\n        model: Trained Faster R-CNN model.\n        output_path (str): Path to save the output image.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    # Download and load image from URL\n    try:\n        response = requests.get(url)\n        response.raise_for_status()  # Check for request errors\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n    except Exception as e:\n        raise Exception(f\"Failed to load image from URL: {e}\")\n    \n    # Preprocess image\n    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n    \n    # Get predictions\n    with torch.no_grad():\n        predictions = model(image_tensor)[0]\n    \n    # Draw predicted boxes\n    draw = ImageDraw.Draw(image)\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 15)\n    except:\n        font = ImageFont.load_default()\n    \n    for bbox, score in zip(predictions['boxes'], predictions['scores']):\n        x1, y1, x2, y2 = bbox.tolist()\n        draw.rectangle((x1, y1, x2, y2), outline='red', width=2)\n        draw.text((x1, y1 - 15), f'{score:.2f}', fill='red', font=font)\n    \n    # Save output image\n    image.save(output_path)\n    \n    # Display image\n    plt.figure(figsize=(10, 10))\n    plt.imshow(np.array(image))\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:57:10.553867Z","iopub.execute_input":"2025-06-19T06:57:10.554382Z","iopub.status.idle":"2025-06-19T06:57:10.566122Z","shell.execute_reply.started":"2025-06-19T06:57:10.554357Z","shell.execute_reply":"2025-06-19T06:57:10.565088Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ảnh dùng fine-tuned model","metadata":{}},{"cell_type":"code","source":"draw_predicted_bbox_file('/kaggle/input/inria-coco-format/demo.png', model, '/kaggle/working/normal_demo.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:57:15.341438Z","iopub.execute_input":"2025-06-19T06:57:15.342307Z","iopub.status.idle":"2025-06-19T06:57:16.151359Z","shell.execute_reply.started":"2025-06-19T06:57:15.342283Z","shell.execute_reply":"2025-06-19T06:57:16.150560Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Ảnh dùng pretrained_model","metadata":{}},{"cell_type":"code","source":"pretrained_model.eval()\ndraw_predicted_bbox_file('/kaggle/input/inria-coco-format/demo.png', pretrained_model, '/kaggle/working/org_demo.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-19T06:57:44.057364Z","iopub.execute_input":"2025-06-19T06:57:44.058025Z","iopub.status.idle":"2025-06-19T06:57:44.665196Z","shell.execute_reply.started":"2025-06-19T06:57:44.058000Z","shell.execute_reply":"2025-06-19T06:57:44.664377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"draw_predicted_bbox('https://scontent.fsgn3-1.fna.fbcdn.net/v/t39.30808-6/504065779_9486874648084132_138695223547462727_n.jpg?_nc_cat=104&ccb=1-7&_nc_sid=833d8c&_nc_eui2=AeHyG9Gz9ww46K8mhR-o_koQbvb3RlXT0N5u9vdGVdPQ3r4O3Dgh4doMDwgyDXjUZt_Y9AHfYP_RwpGOTe8c7G2N&_nc_ohc=4B6vpFlITmYQ7kNvwHTToBK&_nc_oc=Adl7tE4_xnK_jj4OtUs9Khj1CQGM6Dj5A2aUhqNVqwx65Y_dNj8NpIa9P_UPBRSOu_jFK8YOTTlCVU5hVc58aKwV&_nc_zt=23&_nc_ht=scontent.fsgn3-1.fna&_nc_gid=pUdAQw23gSUUPanBXt3yyg&oh=00_AfNjRZbp7RLrmje6o633WYZwtEndtfpcc4KvHtGXRtfbdw&oe=684C281A', model, '/kaggle/working/image.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-09T01:35:51.115337Z","iopub.execute_input":"2025-06-09T01:35:51.116079Z","iopub.status.idle":"2025-06-09T01:35:53.349160Z","shell.execute_reply.started":"2025-06-09T01:35:51.116056Z","shell.execute_reply":"2025-06-09T01:35:53.348298Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Hàm vẽ các predicted_bbox với đầu vào là đường dẫn của một video","metadata":{}},{"cell_type":"code","source":"def draw_predicted_bbox_video(video_path, model, output_path, display_frames=5):\n    \"\"\"\n    Draw predicted bounding boxes (red) with confidence scores on each frame of a local video,\n    display a few sample frames, and save the output video to output_path.\n    \n    Args:\n        video_path (str): Path to the input video file on local storage (e.g., Kaggle).\n        model: Trained Faster R-CNN model.\n        output_path (str): Path to save the output video.\n        display_frames (int): Number of sample frames to display (default: 5).\n    \n    Returns:\n        bool: True if processing is successful, False otherwise.\n    \"\"\"\n    # Set model to evaluation mode\n    model.eval()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    \n    # Check if video file exists\n    if not os.path.exists(video_path):\n        print(f\"Video file not found: {video_path}\")\n        return False\n    \n    # Open video\n    cap = cv2.VideoCapture(video_path)\n    if not cap.isOpened():\n        print(f\"Failed to open video: {video_path}. Check if the file is a valid video or if OpenCV supports the codec.\")\n        return False\n    \n    # Get video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    print(f\"Video properties: FPS={fps}, Width={width}, Height={height}, Frames={frame_count}\")\n    \n    # Initialize video writer\n    try:\n        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n        if not out.isOpened():\n            raise Exception(\"Failed to initialize video writer. Ensure 'mp4v' codec is supported.\")\n    except Exception as e:\n        print(f\"Error initializing video writer: {e}\")\n        cap.release()\n        return False\n    \n    # Font for confidence scores\n    try:\n        font = ImageFont.truetype(\"arial.ttf\", 15)\n    except:\n        font = ImageFont.load_default()\n    \n    # Process frames\n    sample_frames = []\n    frame_idx = 0\n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n        \n        # Convert frame to PIL Image\n        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        image = Image.fromarray(frame_rgb)\n        image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n        \n        # Get predictions\n        with torch.no_grad():\n            predictions = model(image_tensor)[0]\n        \n        # Draw predicted boxes\n        draw = ImageDraw.Draw(image)\n        for bbox, score in zip(predictions['boxes'], predictions['scores']):\n            x1, y1, x2, y2 = bbox.tolist()\n            draw.rectangle((x1, y1, x2, y2), outline='red', width=2)\n            draw.text((x1, y1 - 15), f'{score:.2f}', fill='red', font=font)\n        \n        # Convert back to OpenCV format\n        frame_out = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n        out.write(frame_out)\n        \n        # Store sample frames for display\n        if frame_idx < display_frames:\n            sample_frames.append(np.array(image))\n        \n        frame_idx += 1\n    \n    # Release resources\n    cap.release()\n    out.release()\n    \n    # Display sample frames\n    if sample_frames:\n        plt.figure(figsize=(15, 5))\n        for i, frame in enumerate(sample_frames):\n            plt.subplot(1, min(display_frames, len(sample_frames)), i + 1)\n            plt.imshow(frame)\n            plt.axis('off')\n            plt.title(f'Frame {i + 1}')\n        plt.show()\n    else:\n        print(\"No frames were processed.\")\n    \n    print(f\"Output video saved to: {output_path}\")\n    return True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:15:37.386014Z","iopub.execute_input":"2025-06-06T11:15:37.386298Z","iopub.status.idle":"2025-06-06T11:15:37.398474Z","shell.execute_reply.started":"2025-06-06T11:15:37.386279Z","shell.execute_reply":"2025-06-06T11:15:37.397744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"success = draw_predicted_bbox_video('/kaggle/input/inria-coco-format/demo_video.mp4', normal_model, '/kaggle/working/demo_video.mp4')\nif success:\n    print(\"Video processing completed successfully!\")\nelse:\n    print(\"Video processing failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-06T11:15:53.420072Z","iopub.execute_input":"2025-06-06T11:15:53.420595Z","iopub.status.idle":"2025-06-06T11:16:49.470606Z","shell.execute_reply.started":"2025-06-06T11:15:53.420570Z","shell.execute_reply":"2025-06-06T11:16:49.469873Z"}},"outputs":[],"execution_count":null}]}